{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mp.weixin.qq.com/s/eo7MjFgs7pjbtRZ3R-DcEQm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os \n",
    "import shutil\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.基础配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0+cu92\n",
      "9.2\n",
      "7301\n",
      "GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# 检测pytorch版本\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新pytorch\n",
    "# conda update pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cb9346d6f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定程序运行在特定GPU卡上\n",
    "# CUDA_VISIBLE_DEVICES=0,1 python train.py\n",
    "# 或在代码中指定\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 判断是否有CUDA支持\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置为cuDNN benchmark模式\n",
    "# benchmark模式会提升计算速度，但是由于计算机中有随机性，每次网络前馈结果略有差异\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# 如果想避免这种结果波动，设置\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清楚GPU存储\n",
    "# 有时Control-C中止运行后GPU存储没有及时释放，需要手动清空，在pytorch内部可以\n",
    "# torch.cuda.empty_cache()\n",
    "# 或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程\n",
    "# ps aux | grep python\n",
    "# kill -9 [pid]\n",
    "# 或者直接重置没有被清空的GPU\n",
    "# nvidia-smi --gpu-reset -i [gpu_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量基本信息\n",
    "tensor.type()   # Data type\n",
    "tensor.size()   # Shape of the tensor. It is a subclass of Python tuple\n",
    "tensor.dim()    # Number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类型转换\n",
    "# Set default tensor type. Float in PyTorch is much faster than double.\n",
    "torch.set_default_dtype(torch.FloatType)\n",
    "# type convertions\n",
    "tensor = tensor.cuda()\n",
    "tensor = tensor.cpu()\n",
    "tensor = tensor.float()\n",
    "tensor = tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor与np.ndarray转换\n",
    "# torch.Tensor -> np.ndarray\n",
    "ndarray = tensor.cpu().numpy()\n",
    "# np.ndarray -> torch.Tensor\n",
    "tensor = torch.from_numpy(ndarray).float()\n",
    "tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor与PIL.Image转换\n",
    "# pytorch中的张量默认采用N×C×H×W的顺序，并且数据范围在[0,1]需要进行转置和规范化\n",
    "# torch.Tensor -> PIL.Image\n",
    "image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255\n",
    "    ).byte().permute(1, 2, 0).cpu().numpy())\n",
    "image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way\n",
    "\n",
    "# PIL.Image -> torch.Tensor.\n",
    "tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))\n",
    "    ).permute(2, 0, 1).float() / 255\n",
    "tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))  # Equivalently way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ndarray与PIL.Image转换\n",
    "# np.ndarray -> PIL.Image\n",
    "image = PIL.Image.fromarray(ndarray.astype(np.uint8))\n",
    "# PIL.Image -> np.ndarray\n",
    "ndarray = np.asarray(PIL.Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从只包含一个元素的张量中提取值，这在训练时统计loss的变化过程中特别有用，否则这将累积计算图，使GPU存储占用量越来越大\n",
    "value = tensor.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量变形，常常需要用于将卷积层特征输入全连接层的情形，相比torch.view,torch.reshape可以自动处理输入张量不连续的情况\n",
    "tensor = torch.reshape(tensor,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱顺序\n",
    "tensor = tensor[torch.randperm(tensor.size(0))] # Shuffle the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 水平翻转，pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以用张量索引实现\n",
    "tensor = tensor[:,:,:,torch.arange(tensor.size(3) -1,-1,-1).long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制张量，有三种复制的方式，对应不同的需求\n",
    "# Operation                 |  New/Shared memory | Still in computation graph |\n",
    "tensor.clone()            # |        New         |          Yes               |\n",
    "tensor.detach()           # |      Shared        |          No                |\n",
    "tensor.detach.clone()()   # |        New         |          No  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接张量，注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，而torch.stack会新增一维，例如当参数是3个10×5的张量，torch.cat的结果是30×5的张量，而torch.stack的结果是3×10×5的张量。\n",
    "tensor = torch.cat(list_of_tensors, dim=0)\n",
    "tensor = torch.stack(list_of_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整数标记转换成独热（one-hot）编码,pytorch中的标记默认从0开始\n",
    "N  = tensor.size(0)\n",
    "one_hot = torch.zeros(N,num_classes).long()\n",
    "one_hot.scatter_(dim=1,index=torch.unsqueeze(tensor,dim=1),src=torch.ones(N.num_classes).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到非零/零元素\n",
    "torch.nonzero(tensor)\n",
    "torch.nonzero(tensor == 0)\n",
    "torch.nonzero(tensor).size(0)\n",
    "torch.nonzero(tensor == 0).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断两个张量相等\n",
    "torch.allclose(tensor1,tensor2) # float tensor\n",
    "torch.equal(tensor1,tensor2) # int tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量扩展\n",
    "# # Expand tensor of shape 64*512 to shape 64*512*7*7.\n",
    "torch.reshape(tensor,(64,512,1,1)).expand(64, 512, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩阵乘法\n",
    "# Matrix multiplication: (m*n) * (n*p) -> (m*p).\n",
    "result = torch.mm(tensor1, tensor2)\n",
    "\n",
    "# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p).\n",
    "result = torch.bmm(tensor1, tensor2)\n",
    "\n",
    "# Element-wise multiplication.\n",
    "result = tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两组数据之间的欧式距离\n",
    "dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2,dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层 最常用的卷积层配置是\n",
    "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP(Global average pooling)层\n",
    "gap = torch.nn.AdaptiveAvgPool2d(output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双线性汇合（bilinear pooling）\n",
    "X = torch.reshape(N, D, H * W)                        # Assume X has shape N*D*H*W\n",
    "X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling\n",
    "assert X.size() == (N, D, D)\n",
    "X = torch.reshape(X, (N, D * D))\n",
    "X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization\n",
    "X = torch.nn.functional.normalize(X)                  # L2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多卡同步BN(Batch normalization)\n",
    "sync_bn = torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\n",
    "# 将已有网络的所有BN层改为同步BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类型BN滑动平均，如果要实现类型BN滑动平均的操作，在forward函数中要使用原地（inplace）操作给滑动平均赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
